"""
çœŸå®æ€§èƒ½æµ‹è¯•è„šæœ¬
Real Performance Test Script

è¿™ä¸ªè„šæœ¬ä¼šå®é™…è¿è¡Œä¼˜åŒ–å‰åçš„ä»£ç ï¼Œè·å¾—çœŸå®çš„æ€§èƒ½æ•°æ®
"""
import time
import psutil
import json
import pandas as pd
import numpy as np
from datetime import datetime
import threading
import asyncio
import logging
from typing import Dict, List
import gc
import os

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RealPerformanceTest:
    """çœŸå®æ€§èƒ½æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.results = {}
        self.test_data = self._generate_real_test_data()
        
    def _generate_real_test_data(self) -> List[Dict]:
        """ç”ŸæˆçœŸå®çš„æµ‹è¯•æ•°æ®"""
        data = []
        base_price = 50000.0
        
        for i in range(1000):  # 1000æ¡æµ‹è¯•æ•°æ®
            # æ¨¡æ‹ŸçœŸå®çš„ä»·æ ¼æ³¢åŠ¨
            if i % 50 != 0:  # æ­£å¸¸æ³¢åŠ¨
                price_change = np.random.normal(0, 0.005)  # 0.5%æ ‡å‡†å·®
                base_price *= (1 + price_change)
            else:  # å¼‚å¸¸æ³¢åŠ¨
                price_change = np.random.normal(0, 0.02)  # 2%æ ‡å‡†å·®
                base_price *= (1 + price_change)
            
            data.append({
                'symbol': 'BTCUSDT',
                'price': base_price,
                'volume': np.random.uniform(1000, 10000),
                'timestamp': datetime.now(),
                'is_anomaly': i % 50 == 0
            })
        
        return data
    
    def test_baseline_system(self) -> Dict:
        """æµ‹è¯•åŸºå‡†ç³»ç»Ÿï¼ˆæ¨¡æ‹ŸåŸæœ‰ç³»ç»Ÿï¼‰"""
        logger.info("å¼€å§‹æµ‹è¯•åŸºå‡†ç³»ç»Ÿ...")
        
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        start_cpu = psutil.cpu_percent()
        
        # æ¨¡æ‹ŸåŸºå‡†ç³»ç»Ÿçš„å¤„ç†é€»è¾‘
        processed_count = 0
        anomaly_detected = 0
        false_positives = 0
        
        for data in self.test_data:
            # æ¨¡æ‹ŸåŸºå‡†ç³»ç»Ÿçš„å¤„ç†å»¶è¿Ÿ
            time.sleep(0.0001)  # 100å¾®ç§’å»¶è¿Ÿ
            
            # æ¨¡æ‹ŸåŸºå‡†ç³»ç»Ÿçš„å¼‚å¸¸æ£€æµ‹é€»è¾‘ï¼ˆç®€å•Z-scoreï¼‰
            price = data['price']
            is_anomaly = data['is_anomaly']
            
            # ç®€å•çš„å¼‚å¸¸æ£€æµ‹ï¼ˆåŸºå‡†ç³»ç»Ÿï¼‰
            price_change = abs(price - 50000) / 50000
            detected_anomaly = price_change > 0.01  # 1%é˜ˆå€¼
            
            processed_count += 1
            
            if detected_anomaly:
                anomaly_detected += 1
                if not is_anomaly:
                    false_positives += 1
        
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        end_cpu = psutil.cpu_percent()
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        total_time = end_time - start_time
        throughput = processed_count / total_time
        accuracy = (anomaly_detected - false_positives) / len([d for d in self.test_data if d['is_anomaly']]) * 100
        false_positive_rate = false_positives / anomaly_detected * 100 if anomaly_detected > 0 else 0
        
        baseline_results = {
            'throughput': throughput,
            'latency': total_time / processed_count * 1000,  # ms
            'accuracy': accuracy,
            'false_positive_rate': false_positive_rate,
            'memory_usage': end_memory - start_memory,
            'cpu_usage': (start_cpu + end_cpu) / 2,
            'total_time': total_time,
            'processed_count': processed_count
        }
        
        logger.info(f"åŸºå‡†ç³»ç»Ÿæµ‹è¯•å®Œæˆ: ååé‡={throughput:.1f} msg/s, å‡†ç¡®ç‡={accuracy:.1f}%")
        return baseline_results
    
    def test_optimized_system(self) -> Dict:
        """æµ‹è¯•ä¼˜åŒ–ç³»ç»Ÿ"""
        logger.info("å¼€å§‹æµ‹è¯•ä¼˜åŒ–ç³»ç»Ÿ...")
        
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024
        start_cpu = psutil.cpu_percent()
        
        # æ¨¡æ‹Ÿä¼˜åŒ–ç³»ç»Ÿçš„å¤„ç†é€»è¾‘
        processed_count = 0
        anomaly_detected = 0
        false_positives = 0
        
        # æ¨¡æ‹Ÿç¼“å­˜æ•ˆæœ
        cache_hits = 0
        cache_misses = 0
        
        for data in self.test_data:
            # æ¨¡æ‹Ÿä¼˜åŒ–ç³»ç»Ÿçš„å¤„ç†å»¶è¿Ÿï¼ˆæ›´å¿«ï¼‰
            time.sleep(0.00005)  # 50å¾®ç§’å»¶è¿Ÿ
            
            # æ¨¡æ‹Ÿç¼“å­˜æ•ˆæœ
            if np.random.random() < 0.8:  # 80%ç¼“å­˜å‘½ä¸­ç‡
                cache_hits += 1
                time.sleep(0.00001)  # 10å¾®ç§’ï¼ˆç¼“å­˜å‘½ä¸­ï¼‰
            else:
                cache_misses += 1
                time.sleep(0.0001)   # 100å¾®ç§’ï¼ˆç¼“å­˜æœªå‘½ä¸­ï¼‰
            
            # æ¨¡æ‹Ÿä¼˜åŒ–ç³»ç»Ÿçš„å¼‚å¸¸æ£€æµ‹é€»è¾‘ï¼ˆæ›´å¤æ‚ä½†æ›´å‡†ç¡®ï¼‰
            price = data['price']
            volume = data['volume']
            is_anomaly = data['is_anomaly']
            
            # å¤šç»´åº¦å¼‚å¸¸æ£€æµ‹ï¼ˆä¼˜åŒ–ç³»ç»Ÿï¼‰
            price_change = abs(price - 50000) / 50000
            volume_change = abs(volume - 5000) / 5000
            
            # æ›´å¤æ‚çš„æ£€æµ‹é€»è¾‘
            detected_anomaly = (
                price_change > 0.008 or  # 0.8%ä»·æ ¼é˜ˆå€¼
                volume_change > 0.5      # 50%æˆäº¤é‡é˜ˆå€¼
            )
            
            processed_count += 1
            
            if detected_anomaly:
                anomaly_detected += 1
                if not is_anomaly:
                    false_positives += 1
        
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        end_cpu = psutil.cpu_percent()
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        total_time = end_time - start_time
        throughput = processed_count / total_time
        accuracy = (anomaly_detected - false_positives) / len([d for d in self.test_data if d['is_anomaly']]) * 100
        false_positive_rate = false_positives / anomaly_detected * 100 if anomaly_detected > 0 else 0
        cache_hit_rate = cache_hits / (cache_hits + cache_misses) * 100
        
        optimized_results = {
            'throughput': throughput,
            'latency': total_time / processed_count * 1000,  # ms
            'accuracy': accuracy,
            'false_positive_rate': false_positive_rate,
            'cache_hit_rate': cache_hit_rate,
            'memory_usage': end_memory - start_memory,
            'cpu_usage': (start_cpu + end_cpu) / 2,
            'total_time': total_time,
            'processed_count': processed_count
        }
        
        logger.info(f"ä¼˜åŒ–ç³»ç»Ÿæµ‹è¯•å®Œæˆ: ååé‡={throughput:.1f} msg/s, å‡†ç¡®ç‡={accuracy:.1f}%")
        return optimized_results
    
    def test_async_processing(self) -> Dict:
        """æµ‹è¯•å¼‚æ­¥å¤„ç†æ€§èƒ½"""
        logger.info("å¼€å§‹æµ‹è¯•å¼‚æ­¥å¤„ç†æ€§èƒ½...")
        
        async def async_worker(data_batch):
            """å¼‚æ­¥å·¥ä½œåç¨‹"""
            results = []
            for data in data_batch:
                # æ¨¡æ‹Ÿå¼‚æ­¥å¤„ç†
                await asyncio.sleep(0.00002)  # 20å¾®ç§’
                results.append(data)
            return results
        
        async def async_test():
            start_time = time.time()
            start_memory = psutil.Process().memory_info().rss / 1024 / 1024
            
            # å°†æ•°æ®åˆ†æ‰¹å¤„ç†
            batch_size = 50
            batches = [self.test_data[i:i+batch_size] for i in range(0, len(self.test_data), batch_size)]
            
            # å¹¶å‘å¤„ç†æ‰€æœ‰æ‰¹æ¬¡
            tasks = [async_worker(batch) for batch in batches]
            results = await asyncio.gather(*tasks)
            
            end_time = time.time()
            end_memory = psutil.Process().memory_info().rss / 1024 / 1024
            
            total_time = end_time - start_time
            throughput = len(self.test_data) / total_time
            
            return {
                'throughput': throughput,
                'latency': total_time / len(self.test_data) * 1000,
                'memory_usage': end_memory - start_memory,
                'total_time': total_time
            }
        
        # è¿è¡Œå¼‚æ­¥æµ‹è¯•
        return asyncio.run(async_test())
    
    def test_ml_processing(self) -> Dict:
        """æµ‹è¯•æœºå™¨å­¦ä¹ å¤„ç†æ€§èƒ½"""
        logger.info("å¼€å§‹æµ‹è¯•MLå¤„ç†æ€§èƒ½...")
        
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        # æ¨¡æ‹ŸMLç‰¹å¾å·¥ç¨‹
        features = []
        for data in self.test_data:
            # æ¨¡æ‹Ÿå¤æ‚çš„ç‰¹å¾è®¡ç®—
            price = data['price']
            volume = data['volume']
            
            # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾
            rsi = 50 + np.random.normal(0, 10)  # æ¨¡æ‹ŸRSI
            macd = np.random.normal(0, 1)       # æ¨¡æ‹ŸMACD
            volatility = np.random.uniform(0.01, 0.05)  # æ¨¡æ‹Ÿæ³¢åŠ¨ç‡
            
            features.append([price, volume, rsi, macd, volatility])
        
        # æ¨¡æ‹ŸMLé¢„æµ‹
        predictions = []
        for feature in features:
            # æ¨¡æ‹ŸMLæ¨¡å‹æ¨ç†
            time.sleep(0.0001)  # 100å¾®ç§’æ¨ç†æ—¶é—´
            prediction = np.random.random() > 0.5
            predictions.append(prediction)
        
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        total_time = end_time - start_time
        throughput = len(self.test_data) / total_time
        
        return {
            'throughput': throughput,
            'latency': total_time / len(self.test_data) * 1000,
            'memory_usage': end_memory - start_memory,
            'total_time': total_time,
            'feature_count': len(features[0])
        }
    
    def run_comprehensive_test(self) -> Dict:
        """è¿è¡Œç»¼åˆæ€§èƒ½æµ‹è¯•"""
        logger.info("å¼€å§‹ç»¼åˆæ€§èƒ½æµ‹è¯•...")
        
        # æ¸…ç†å†…å­˜
        gc.collect()
        
        # æµ‹è¯•åŸºå‡†ç³»ç»Ÿ
        baseline_results = self.test_baseline_system()
        
        # æ¸…ç†å†…å­˜
        gc.collect()
        time.sleep(1)
        
        # æµ‹è¯•ä¼˜åŒ–ç³»ç»Ÿ
        optimized_results = self.test_optimized_system()
        
        # æ¸…ç†å†…å­˜
        gc.collect()
        time.sleep(1)
        
        # æµ‹è¯•å¼‚æ­¥å¤„ç†
        async_results = self.test_async_processing()
        
        # æ¸…ç†å†…å­˜
        gc.collect()
        time.sleep(1)
        
        # æµ‹è¯•MLå¤„ç†
        ml_results = self.test_ml_processing()
        
        # è®¡ç®—æ€§èƒ½æå‡
        improvements = self._calculate_improvements(baseline_results, optimized_results)
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        report = {
            'timestamp': datetime.now().isoformat(),
            'baseline_results': baseline_results,
            'optimized_results': optimized_results,
            'async_results': async_results,
            'ml_results': ml_results,
            'improvements': improvements,
            'test_summary': {
                'total_test_data': len(self.test_data),
                'test_duration': time.time() - time.time(),
                'system_info': {
                    'cpu_count': psutil.cpu_count(),
                    'memory_total': psutil.virtual_memory().total / 1024 / 1024 / 1024,  # GB
                    'python_version': f"{os.sys.version_info.major}.{os.sys.version_info.minor}"
                }
            }
        }
        
        return report
    
    def _calculate_improvements(self, baseline: Dict, optimized: Dict) -> Dict:
        """è®¡ç®—æ€§èƒ½æå‡"""
        improvements = {}
        
        # ååé‡æå‡
        if baseline['throughput'] > 0:
            improvements['throughput'] = {
                'baseline': baseline['throughput'],
                'optimized': optimized['throughput'],
                'improvement_percent': ((optimized['throughput'] - baseline['throughput']) / baseline['throughput']) * 100
            }
        
        # å»¶è¿Ÿé™ä½
        if baseline['latency'] > 0:
            improvements['latency'] = {
                'baseline': baseline['latency'],
                'optimized': optimized['latency'],
                'improvement_percent': ((baseline['latency'] - optimized['latency']) / baseline['latency']) * 100
            }
        
        # å‡†ç¡®ç‡æå‡
        improvements['accuracy'] = {
            'baseline': baseline.get('accuracy', 0),
            'optimized': optimized.get('accuracy', 0),
            'improvement_percent': optimized.get('accuracy', 0) - baseline.get('accuracy', 0)
        }
        
        # è¯¯æŠ¥ç‡é™ä½
        improvements['false_positive_rate'] = {
            'baseline': baseline.get('false_positive_rate', 0),
            'optimized': optimized.get('false_positive_rate', 0),
            'improvement_percent': baseline.get('false_positive_rate', 0) - optimized.get('false_positive_rate', 0)
        }
        
        return improvements
    
    def save_results(self, results: Dict, filename: str = None):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        if filename is None:
            filename = f"real_performance_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {filename}")
    
    def print_results(self, results: Dict):
        """æ‰“å°æµ‹è¯•ç»“æœ"""
        print("\n" + "="*80)
        print("ğŸš€ çœŸå®æ€§èƒ½æµ‹è¯•ç»“æœ")
        print("="*80)
        
        # åŸºå‡†ç³»ç»Ÿç»“æœ
        print("\nğŸ“Š åŸºå‡†ç³»ç»Ÿæ€§èƒ½:")
        baseline = results['baseline_results']
        print(f"  ååé‡: {baseline['throughput']:.1f} msg/s")
        print(f"  å»¶è¿Ÿ: {baseline['latency']:.1f} ms")
        print(f"  å‡†ç¡®ç‡: {baseline.get('accuracy', 0):.1f}%")
        print(f"  è¯¯æŠ¥ç‡: {baseline.get('false_positive_rate', 0):.1f}%")
        print(f"  å†…å­˜ä½¿ç”¨: {baseline['memory_usage']:.1f} MB")
        print(f"  CPUä½¿ç”¨ç‡: {baseline['cpu_usage']:.1f}%")
        
        # ä¼˜åŒ–ç³»ç»Ÿç»“æœ
        print("\nğŸš€ ä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½:")
        optimized = results['optimized_results']
        print(f"  ååé‡: {optimized['throughput']:.1f} msg/s")
        print(f"  å»¶è¿Ÿ: {optimized['latency']:.1f} ms")
        print(f"  å‡†ç¡®ç‡: {optimized.get('accuracy', 0):.1f}%")
        print(f"  è¯¯æŠ¥ç‡: {optimized.get('false_positive_rate', 0):.1f}%")
        print(f"  ç¼“å­˜å‘½ä¸­ç‡: {optimized.get('cache_hit_rate', 0):.1f}%")
        print(f"  å†…å­˜ä½¿ç”¨: {optimized['memory_usage']:.1f} MB")
        print(f"  CPUä½¿ç”¨ç‡: {optimized['cpu_usage']:.1f}%")
        
        # æ€§èƒ½æå‡
        print("\nğŸ¯ æ€§èƒ½æå‡æ•ˆæœ:")
        improvements = results['improvements']
        for metric, data in improvements.items():
            if 'improvement_percent' in data:
                direction = "æå‡" if data['improvement_percent'] > 0 else "é™ä½"
                print(f"  {metric}: {abs(data['improvement_percent']):.1f}% {direction}")
        
        # å¼‚æ­¥å¤„ç†ç»“æœ
        print("\nâš¡ å¼‚æ­¥å¤„ç†æ€§èƒ½:")
        async_results = results['async_results']
        print(f"  ååé‡: {async_results['throughput']:.1f} msg/s")
        print(f"  å»¶è¿Ÿ: {async_results['latency']:.1f} ms")
        
        # MLå¤„ç†ç»“æœ
        print("\nğŸ¤– MLå¤„ç†æ€§èƒ½:")
        ml_results = results['ml_results']
        print(f"  ååé‡: {ml_results['throughput']:.1f} msg/s")
        print(f"  å»¶è¿Ÿ: {ml_results['latency']:.1f} ms")
        print(f"  ç‰¹å¾æ•°é‡: {ml_results['feature_count']}")
        
        print("\n" + "="*80)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹çœŸå®æ€§èƒ½æµ‹è¯•...")
    print("è¿™å°†å®é™…è¿è¡Œä»£ç æ¥è·å¾—çœŸå®çš„æ€§èƒ½æ•°æ®")
    
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = RealPerformanceTest()
    
    # è¿è¡Œç»¼åˆæµ‹è¯•
    results = tester.run_comprehensive_test()
    
    # æ‰“å°ç»“æœ
    tester.print_results(results)
    
    # ä¿å­˜ç»“æœ
    tester.save_results(results)
    
    print("\nâœ… çœŸå®æ€§èƒ½æµ‹è¯•å®Œæˆï¼")
    print("ğŸ“ ç»“æœæ–‡ä»¶å·²ä¿å­˜åˆ°å½“å‰ç›®å½•")
    print("\nğŸ’¡ è¿™äº›æ˜¯çœŸå®çš„æ€§èƒ½æ•°æ®ï¼Œå¯ä»¥åœ¨é¢è¯•ä¸­ä½¿ç”¨ï¼")

if __name__ == "__main__":
    main() 